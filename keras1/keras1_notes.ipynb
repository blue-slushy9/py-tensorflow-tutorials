{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blue-slushy9/py-tensorflow-tutorials/blob/main/keras_beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXYX3SszEjr4",
        "outputId": "0339e494-efc5-4e07-b524-0fdd2ac6a000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "# MNIST refers to the Modified National Institute of Standards and Technology\n",
        "# database, it is a large database of handwritten digits that is commonly used\n",
        "# for training various image processing systems, particularly in the field of\n",
        "# machine learning; the below command is used to load the module and access the\n",
        "# MNIST dataset;\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# The returned values in the ()'s are tuples, x_train & x_test are arrays\n",
        "# containing the images, and y_train & y_test are arrays containing the labels\n",
        "(x_train, y_train), (x_test, y_test) mnist.load_data()\n",
        "\n",
        "# Performs data normalization on the pixel values of the images in the training\n",
        "# and testing datasets; dividing each pixel value by 255.0, the pixel values in\n",
        "# typical image datasets like MNIST range from 0 to 255, representing the\n",
        "# intensity of the pixel (0 for black, 255 for white); dividing by 255 scales\n",
        "# these values to the range [0, 1], which is often beneficial for training\n",
        "# neural networks. Normalization helps in ensuring that the features\n",
        "# (pixel values) are on a similar scale, which can help improve convergence\n",
        "# during training; it's important to apply the same preprocessing steps to the\n",
        "# testing data as to the training data to ensure consistency and fair\n",
        "# evaluation of the model's performance;\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Build a tf.keras.Sequential model:\n",
        "# Sequential is a type of model that represents a linear stack of layers, it is\n",
        "# a straightforward way to build neural networks where each layer has exactly\n",
        "# one input tensor and one output tensor; \n",
        "# 1) create a Sequential model object;\n",
        "model = tf.keras.models.Sequential([\n",
        "    # 2) Add layers to it one by one, these can be instances of various layers\n",
        "    # provided by Keras; e.g. Flatten, which converts multi-dimensional data\n",
        "    # into a one-dimensional array; input_shape value indicates images with \n",
        "    # dimensions 28x28, which are the dimensions of the images in MNIST;\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    # 3) Each layer added to the Sequential model gets stacked one after the\n",
        "    # other, forming a linear stack of layers; Dense is a fully connected \n",
        "    # neural network layer, i.e. every neuron in the current layer is connected\n",
        "    # to every neuron in the previous layer, and each connection has its own\n",
        "    # weight parameter that will be learned during training; 128 is the number\n",
        "    # of neurons; the activation function is ReLU (Rectified Linear Unit), i.e.\n",
        "    # it returns the input directly if it's positive and returns zero otherwise;\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    # Dropout is a regularization technique used to prevent overfitting in\n",
        "    # neural networks; overfitting occurs when a model learns to memorize the\n",
        "    # training data instead of learning the underlying patterns, resulting in\n",
        "    # poor generalization to unseen data; dropout helps address this issue by\n",
        "    # randomly dropping (i.e. setting to zero) a fraction of input units during\n",
        "    # training, which forces the network to reduce reliance on any individual\n",
        "    # neuron; a dropout value of 0.2 means 20% of the input units will be\n",
        "    # randomly set to zero during training;\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    # If no activation function is specified, the Dense layer will apply a\n",
        "    # linear activation function to the output of each neuron by default, i.e.\n",
        "    # the output of each neuron will be a weighted sum of its inputs without any\n",
        "    # non-linearity applied to it;\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Performs inference using the trained model on a single input data point from\n",
        "# the training set x_train; x_train[:1] selects the first data point from the\n",
        "# training set x_train, it is common to use slices of datasets for testing;\n",
        "# model(x_train[:1]) passes the selected input data point to the model for\n",
        "# inference; in Keras, models are callable, so you can directly pass input data\n",
        "# to a model instance to obtain predictions; .numpy() converts the output of\n",
        "# the model (TensorFlow tensor) into a NumPy array; the numpy() method is used\n",
        "# here to extract the actual values of the predictions for further processing;\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "# Simply evaluates the variable predictions in the current context and outputs\n",
        "# its value; in Python, when you type a variable name in the interactive \n",
        "# interpreter or in a Jupyter Notebook cell without assigning it to anything,\n",
        "# the interpreter or notebook will display the value of that variable if it's\n",
        "# available in the current context; i.e. same thing as 'print(predictions)';\n",
        "predictions\n",
        "\n",
        "# Converts the above logits to probabilities for each class; applies the\n",
        "# softmax function to the array of predictions and converts the result into a\n",
        "# NumPy array; softmax is an activation function, predictions is the input;\n",
        "# softmax is commonly used in classification problems to convert raw prediction\n",
        "# scores into probabilities, it squashes the input values between 0 & 1 and \n",
        "# normalizes them so they add up to 1;\n",
        "tf.nn.softmax(predictions).numpy()\n",
        "\n",
        "# Define a loss function for training, the loss function takes a vector of\n",
        "# ground truth values and a vector of logits and returns a scalar loss for each\n",
        "# example; this loss is equal to the negative log probability of the true class,\n",
        "# the loss is zero if the model is sure of the correct class; tf.keras...entropy\n",
        "# creates an instance of the SCC loss function class provided by TF's Keras API,\n",
        "# this loss function is commonly used for multi-class classification tasks where\n",
        "# the target labels are integers; from...True specifies that the input to the\n",
        "# loss function ('logits') are unnormalized prediction scores (logits) rather\n",
        "# than probabilities; from...True specifies for the loss function to internally\n",
        "# apply the softmax functions to the input logits before computing cross-entropy\n",
        "# loss;\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Calling the loss_fn() defined above with arguments, it represents the loss\n",
        "# function to be ued for computing the loss between the true labels and the\n",
        "# predictions; y_train[:1] is the true labels for a single training example\n",
        "# (usually the ground truth labels), predictions represents the predicted\n",
        "# labels for the same example; \n",
        "loss_fn(y_train[:1], predictions).numpy()\n",
        "\n",
        "# Configure and compile the model for training; optim...'adam' specifies the\n",
        "# optimizer to be used during training, Adam is a popular choice for training\n",
        "# neural networks due to its adaptive learning rate properties and efficiency;\n",
        "# loss function we are using was defined above; metrics=['accuracy'] specifies\n",
        "# the evaluation metrics to be using during training and validation, in this\n",
        "# case accuracy is the metric we will monitor during training, which computes\n",
        "# the accuracy of the model on the training data; additional metrics can be\n",
        "# added as needed; \n",
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4) Once the model is constructed, you compile it by specifying the loss\n",
        "# function, optimizer, and any evaluation metrics; then you can train the model\n",
        "# on your data using the fit() method;\n",
        "\n",
        "# Adjust model parameters and minimize loss; model we defined earlier consists\n",
        "# of input layers, hidden layers, an output layer, and has been compiled with\n",
        "# an optimizer, loss function, and possibly additional metrics; fit() is a\n",
        "# method provided by Keras models for training models on a given dataset, the\n",
        "# fit method iteratively trains the model using the specified training data for\n",
        "# a certain number of epochs; again, x_train is input training data and y_train\n",
        "# is target training data containing the corresponding labels or target values\n",
        "# for the input samples in x_train; an epoch is a complete pass through the\n",
        "# entire training dataset, in this case 5; during training, the model will\n",
        "# adjust its weights and biases based on the training data and the optimization\n",
        "# algorithm to minimize the loss function and improve its performance on the\n",
        "# given task, e.g. classification or regression; after training completes, the\n",
        "# model will have learned patterns and relationships in the training data that\n",
        "# enable it to make predictions on new, unseen data;\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Checks the model's performance, usually on a separate validation set or test \n",
        "# set than what it was trained on; the Keras evaluate method computes the loss\n",
        "# value and any specified metrics on the test data and returns them as output;\n",
        "# verbose controls the verbosity of the evaluation process, 2 means progress\n",
        "# bars will be displayed during evaluation, with one progress bar per epoch;\n",
        "# evaluate returns a list of test results;\n",
        "model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "# 5) After training, you can use the model to make predictions on new data\n",
        "# using the predict() method; the difference between evaluate() and predict()\n",
        "# is that evaluate is used for assessing the model's performance on a dataset\n",
        "# by computing loss and metrics, while predict() is used for obtaining\n",
        "# predictions from the model for a given input dataset;\n",
        "\n",
        "# If you want your model to return a probability, you can wrap the trained\n",
        "# model and attach the softmax to it; defines a new Keras model called\n",
        "# prob...model composed of two sequential layers: model (which we defined above) \n",
        "# and the Softmax layer, which converts the raw output (logits) into\n",
        "# probabilities; it applies the softmax activation function to the output of\n",
        "# the previous model's layers, which normalizes the output into a probability\n",
        "# distribution over the predicted classes; each output neuron's activation\n",
        "# represents the probability that the input belongs to the corresponding class;\n",
        "# the Softmax layer is typicall used in classification tasks to obtain class\n",
        "# probabilities; this type of model is useful when you want to obtain the class\n",
        "# probabilities instead of raw scores or when you want to use the model for\n",
        "# inference on new data where class probabilities are needed;\n",
        "probability_model = tf.keras.Sequential([\n",
        "    model,\n",
        "    tf.keras.layers.Softmax()\n",
        "])\n",
        "# i.e. print(probability_model(x_test[:5]))\n",
        "probability_model(x_test[:5])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMfEzdkS8bhasrf6nbpb3Lh",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
